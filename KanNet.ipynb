{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "cellView": "form",
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-04-01T15:02:12.474098Z",
     "iopub.status.busy": "2025-04-01T15:02:12.472338Z",
     "iopub.status.idle": "2025-04-01T15:02:35.711079Z",
     "shell.execute_reply": "2025-04-01T15:02:35.709506Z",
     "shell.execute_reply.started": "2025-04-01T15:02:12.474042Z"
    },
    "id": "_yWe10UGiD6B",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#@title Installs\n",
    "#!pip install torch\n",
    "#!pip install timm\n",
    "#!pip install tqdm\n",
    "#!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T16:23:45.778975Z",
     "start_time": "2025-04-01T16:23:45.560610Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-04-01T15:02:35.716085Z",
     "iopub.status.busy": "2025-04-01T15:02:35.715582Z",
     "iopub.status.idle": "2025-04-01T15:02:35.978472Z",
     "shell.execute_reply": "2025-04-01T15:02:35.976446Z",
     "shell.execute_reply.started": "2025-04-01T15:02:35.716043Z"
    },
    "id": "7b33gannsqn_",
    "outputId": "ac86d449-b348-48e7-87b7-fb8e2b27750c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Apr  3 02:12:13 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce GTX 1060 6GB    Off | 00000000:09:00.0  On |                  N/A |\n",
      "| 25%   50C    P5              12W / 180W |    597MiB /  6144MiB |     27%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      1123      G   /usr/lib/xorg/Xorg                          162MiB |\n",
      "|    0   N/A  N/A      1563      G   /usr/bin/kwin_x11                            67MiB |\n",
      "|    0   N/A  N/A      1630      G   /usr/bin/plasmashell                         29MiB |\n",
      "|    0   N/A  N/A      4664      G   ...erProcess --variations-seed-version      274MiB |\n",
      "|    0   N/A  N/A     42749      G   ...eee68fbe36e507b486c2b5b1ce695adde21       55MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T16:23:53.306849Z",
     "start_time": "2025-04-01T16:23:46.012213Z"
    },
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-04-01T15:02:35.983118Z",
     "iopub.status.busy": "2025-04-01T15:02:35.982588Z",
     "iopub.status.idle": "2025-04-01T15:02:35.996060Z",
     "shell.execute_reply": "2025-04-01T15:02:35.994186Z",
     "shell.execute_reply.started": "2025-04-01T15:02:35.983070Z"
    },
    "id": "QgBsmjSYh9cQ",
    "outputId": "11bcf2d2-9e8f-4da3-8bd5-6e881fd0af0f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dejan/Documents/KANNet-main/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#@title Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import dataset\n",
    "\n",
    "\n",
    "import argparse\n",
    "import timm\n",
    "from tqdm import tqdm\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jhTMlBUXhx-w"
   },
   "source": [
    "# Components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T15:02:35.999829Z",
     "iopub.status.busy": "2025-04-01T15:02:35.999309Z",
     "iopub.status.idle": "2025-04-01T15:02:36.020664Z",
     "shell.execute_reply": "2025-04-01T15:02:36.018684Z",
     "shell.execute_reply.started": "2025-04-01T15:02:35.999782Z"
    },
    "id": "_8x6IinRj1Dj"
   },
   "outputs": [],
   "source": [
    "#@title ReluKANLayer\n",
    "class ReluKANLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size: int, output_size: int, g: int, k: int,  is_train: bool = False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.g, self.k, self.r = g, k, 4*g*g / ((k+1)*(k+1))\n",
    "        self.input_size, self.output_size = input_size, output_size\n",
    "        phase_low = np.arange(-k, g) / g\n",
    "        phase_height = phase_low + (k+1) / g\n",
    "        self.phase_low = nn.Parameter(torch.Tensor(np.array([phase_low for i in range(input_size)])), requires_grad=is_train)\n",
    "        self.phase_height = nn.Parameter(torch.Tensor(np.array([phase_height for i in range(input_size)])), requires_grad=is_train)\n",
    "        self.equal_size_conv = nn.Conv2d(1, output_size, (g+k, input_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x_expanded = x.unsqueeze(2).expand(-1, -1, self.phase_low.size(1))\n",
    "        #x1 = torch.relu(x_expanded - self.phase_low)\n",
    "        #x2 = torch.relu(self.phase_height - x_expanded)\n",
    "\n",
    "        # x: (batch_size, input_size)\n",
    "        x_unsqueezed = x.unsqueeze(2)  # Now (batch_size, input_size, 1)\n",
    "        # Unsqueeze parameters to (1, input_size, phase_size) so they broadcast correctly.\n",
    "        phase_low = self.phase_low.unsqueeze(0)\n",
    "        phase_height = self.phase_height.unsqueeze(0)\n",
    "\n",
    "        x1 = torch.relu(x_unsqueezed - phase_low)\n",
    "        x2 = torch.relu(phase_height - x_unsqueezed)\n",
    "\n",
    "        x = x1 * x2 * self.r\n",
    "        x = x * x\n",
    "        x = x.reshape((len(x), 1, self.g + self.k, self.input_size))\n",
    "        x = self.equal_size_conv(x)\n",
    "        x = x.reshape((len(x), self.output_size, 1))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2025-04-01T15:02:36.026982Z",
     "iopub.status.busy": "2025-04-01T15:02:36.026553Z",
     "iopub.status.idle": "2025-04-01T15:02:36.046419Z",
     "shell.execute_reply": "2025-04-01T15:02:36.044859Z",
     "shell.execute_reply.started": "2025-04-01T15:02:36.026948Z"
    },
    "id": "eX_1PX_gj7Dd"
   },
   "outputs": [],
   "source": [
    "#@title ReluKANOperator2d\n",
    "class ReluKANOperator2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size,\n",
    "                 stride=1, padding=0, dilation=1, groups=1, bias=True,\n",
    "                 g=None, k=None, kan_module_constructor=None):\n",
    "        \"\"\"\\\n",
    "        Parameters:\n",
    "          - in_channels: Number of channels in the input.\n",
    "          - out_channels: Number of output channels (each will have its own KAN module).\n",
    "          - kernel_size: Kernel size (int or tuple).\n",
    "          - stride, padding, dilation: Convolution parameters.\n",
    "          - groups: Not used in this basic implementation.\n",
    "          - bias: Whether to add a learnable bias.\n",
    "          - g, k: Parameters for ReluKANLayer (if using default constructor).\n",
    "          - kan_module_constructor: Optional callable that accepts the flattened patch size and returns a KAN module.\n",
    "        \"\"\"\n",
    "        super(ReluKANOperator2d, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        # Ensure kernel_size, stride, padding, dilation are tuples.\n",
    "        self.kernel_size = kernel_size if isinstance(kernel_size, tuple) else (kernel_size, kernel_size)\n",
    "        self.stride = stride if isinstance(stride, tuple) else (stride, stride)\n",
    "        self.padding = padding if isinstance(padding, tuple) else (padding, padding)\n",
    "        self.dilation = dilation if isinstance(dilation, tuple) else (dilation, dilation)\n",
    "        self.groups = groups\n",
    "\n",
    "        # The flattened patch size: in_channels * kernel_height * kernel_width.\n",
    "        self.patch_size = in_channels * self.kernel_size[0] * self.kernel_size[1]\n",
    "\n",
    "        # Use the provided kan_module_constructor or default to one that uses ReluKANLayer.\n",
    "        if kan_module_constructor is None:\n",
    "            if g is None or k is None:\n",
    "                raise ValueError(\"Provide g and k parameters for the default ReluKANLayer constructor\")\n",
    "            def default_kan_module_constructor(in_features):\n",
    "                # Each KAN module converts a flattened patch to a scalar (output_size=1).\n",
    "                return ReluKANLayer(input_size=in_features, output_size=1, g=g, k=k)\n",
    "            kan_module_constructor = default_kan_module_constructor\n",
    "\n",
    "        # Create one KAN module per output channel.\n",
    "        self.kan_modules = nn.ModuleList(\n",
    "            [kan_module_constructor(self.patch_size) for _ in range(out_channels)]\n",
    "        )\n",
    "\n",
    "        # Optional bias per output channel.\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.zeros(out_channels))\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, in_channels, H, W)\n",
    "        B, C, H, W = x.shape\n",
    "\n",
    "        # Extract sliding patches; shape: (B, patch_size, L) where L is the number of patches.\n",
    "        patches = F.unfold(x, kernel_size=self.kernel_size, dilation=self.dilation,\n",
    "                           padding=self.padding, stride=self.stride)\n",
    "        # Rearrange to (B, L, patch_size)\n",
    "        patches = patches.transpose(1, 2)\n",
    "        B, L, patch_size = patches.shape\n",
    "\n",
    "        # Flatten the patches to shape (B*L, patch_size) for processing.\n",
    "        patches_reshaped = patches.reshape(B * L, patch_size)\n",
    "\n",
    "        outputs = []\n",
    "        for kan in self.kan_modules:\n",
    "            # Each KAN module processes the flattened patches.\n",
    "            # Expected output shape from ReluKANLayer: (B*L, 1, 1)\n",
    "            out = kan(patches_reshaped)\n",
    "            # Reshape to (B, L)\n",
    "            out = out.view(B, L)\n",
    "            outputs.append(out)\n",
    "\n",
    "        # Stack along a new channel dimension: (B, out_channels, L)\n",
    "        out_tensor = torch.stack(outputs, dim=1)\n",
    "\n",
    "        # Calculate output spatial dimensions.\n",
    "        H_out = (H + 2*self.padding[0] - self.dilation[0]*(self.kernel_size[0] - 1) - 1) // self.stride[0] + 1\n",
    "        W_out = (W + 2*self.padding[1] - self.dilation[1]*(self.kernel_size[1] - 1) - 1) // self.stride[1] + 1\n",
    "\n",
    "        # Reshape to (B, out_channels, H_out, W_out)\n",
    "        out_tensor = out_tensor.view(B, self.out_channels, H_out, W_out)\n",
    "\n",
    "        if self.bias is not None:\n",
    "            out_tensor = out_tensor + self.bias.view(1, -1, 1, 1)\n",
    "\n",
    "        return out_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2025-04-01T15:02:36.049728Z",
     "iopub.status.busy": "2025-04-01T15:02:36.048576Z",
     "iopub.status.idle": "2025-04-01T15:02:36.058704Z",
     "shell.execute_reply": "2025-04-01T15:02:36.057267Z",
     "shell.execute_reply.started": "2025-04-01T15:02:36.049669Z"
    },
    "id": "N5hseolVkHWK"
   },
   "outputs": [],
   "source": [
    "#@title ReluKANBlock\n",
    "class ReluKANBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, g, k, stride=1):\n",
    "        super(ReluKANBlock, self).__init__()\n",
    "        self.layer = ReluKANOperator2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, g=g, k=k)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer(x)\n",
    "        x = self.bn(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T15:04:32.278567Z",
     "iopub.status.busy": "2025-04-01T15:04:32.277341Z",
     "iopub.status.idle": "2025-04-01T15:04:32.291315Z",
     "shell.execute_reply": "2025-04-01T15:04:32.289977Z",
     "shell.execute_reply.started": "2025-04-01T15:04:32.278520Z"
    },
    "id": "FjIksf_0kMMe"
   },
   "outputs": [],
   "source": [
    "#@title ReluKANNetB0\n",
    "class ReluKANNetB0(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 num_classes,\n",
    "                 g=3,\n",
    "                 k=3,\n",
    "                 depth_list=[1, 1, 1, 1],\n",
    "                 channel_configs=[8, 12, 16, 20],\n",
    "                 base_channels=32,\n",
    "                 ):\n",
    "        super(ReluKANNetB0, self).__init__()\n",
    "\n",
    "        self.depth_list = depth_list\n",
    "        self.stem = ReluKANBlock(in_channels, base_channels, g, k, stride=1)\n",
    "\n",
    "        self.stages = nn.ModuleList()\n",
    "        input_channels = base_channels\n",
    "        #channel_configs = [8, 12, 16, 20]\n",
    "        for stage, repeats in enumerate(depth_list):\n",
    "            stage_layers = []\n",
    "            for i in range(repeats):\n",
    "                stride = 2 if i == 0 and stage != 0 else 1\n",
    "                stage_layers.append(ReluKANBlock(input_channels, channel_configs[stage], g, k, stride=stride))\n",
    "                input_channels = channel_configs[stage]\n",
    "            self.stages.append(nn.Sequential(*stage_layers))\n",
    "\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc = nn.Linear(input_channels, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        for stage in self.stages:\n",
    "            x = stage(x)\n",
    "        x = self.avg_pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xJBPAVwJkYCH"
   },
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2025-04-01T15:02:36.079075Z",
     "iopub.status.busy": "2025-04-01T15:02:36.078652Z",
     "iopub.status.idle": "2025-04-01T15:02:36.088762Z",
     "shell.execute_reply": "2025-04-01T15:02:36.087318Z",
     "shell.execute_reply.started": "2025-04-01T15:02:36.079037Z"
    },
    "id": "lJJZIjJ_kb7h"
   },
   "outputs": [],
   "source": [
    "#@title cifar10\n",
    "def cifar10():\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((32, 32)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    train_dataset = torchvision.datasets.CIFAR10(root=\"./data/cifar10/train\", train=True, download=True, transform=transform)\n",
    "    val_dataset = torchvision.datasets.CIFAR10(root=\"./data/cifar10/test\", train=False, download=True, transform=transform)\n",
    "    in_channels = 3\n",
    "    num_classes = 10\n",
    "    return train_dataset, val_dataset, in_channels, num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2025-04-01T15:02:36.091172Z",
     "iopub.status.busy": "2025-04-01T15:02:36.090738Z",
     "iopub.status.idle": "2025-04-01T15:02:36.101371Z",
     "shell.execute_reply": "2025-04-01T15:02:36.099818Z",
     "shell.execute_reply.started": "2025-04-01T15:02:36.091131Z"
    },
    "id": "UfOgo5gTnI1P"
   },
   "outputs": [],
   "source": [
    "#@title cifar100\n",
    "def cifar100():\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((32, 32)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    train_dataset = torchvision.datasets.CIFAR100(root=\"./data/cifar100/train\", train=True, download=True, transform=transform)\n",
    "    val_dataset = torchvision.datasets.CIFAR100(root=\"./data/cifar100/test\", train=False, download=True, transform=transform)\n",
    "    in_channels = 3\n",
    "    num_classes = 100\n",
    "    return train_dataset, val_dataset, in_channels, num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2025-04-01T15:02:36.106970Z",
     "iopub.status.busy": "2025-04-01T15:02:36.106502Z",
     "iopub.status.idle": "2025-04-01T15:02:36.116557Z",
     "shell.execute_reply": "2025-04-01T15:02:36.114214Z",
     "shell.execute_reply.started": "2025-04-01T15:02:36.106932Z"
    },
    "id": "M_2mZyJ6nMO4"
   },
   "outputs": [],
   "source": [
    "#@title stanford_cars\n",
    "def stanford_cars():\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    train_dataset = torchvision.datasets.ImageFolder(root=\"./data/stanford_cars/train\", transform=transform)\n",
    "    val_dataset = torchvision.datasets.ImageFolder(root=\"./data/stanford_cars/test\", transform=transform)\n",
    "    in_channels = 3\n",
    "    num_classes = 196\n",
    "    return train_dataset, val_dataset, in_channels, num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2025-04-01T15:02:36.119391Z",
     "iopub.status.busy": "2025-04-01T15:02:36.118676Z",
     "iopub.status.idle": "2025-04-01T15:02:36.127453Z",
     "shell.execute_reply": "2025-04-01T15:02:36.125957Z",
     "shell.execute_reply.started": "2025-04-01T15:02:36.119337Z"
    },
    "id": "QipZXlQZnOjH"
   },
   "outputs": [],
   "source": [
    "#@title food101\n",
    "def food101():\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    train_dataset = torchvision.datasets.ImageFolder(root=\"./data/food101/train\", transform=transform)\n",
    "    val_dataset = torchvision.datasets.ImageFolder(root=\"./data/food101/test\", transform=transform)\n",
    "    in_channels = 3\n",
    "    num_classes = 101\n",
    "    return train_dataset, val_dataset, in_channels, num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2025-04-01T15:02:36.129712Z",
     "iopub.status.busy": "2025-04-01T15:02:36.129373Z",
     "iopub.status.idle": "2025-04-01T15:02:36.137108Z",
     "shell.execute_reply": "2025-04-01T15:02:36.135380Z",
     "shell.execute_reply.started": "2025-04-01T15:02:36.129673Z"
    },
    "id": "loBSCBzPnQV9"
   },
   "outputs": [],
   "source": [
    "#@title oxford_iiit_pet\n",
    "def oxford_iiit_pet():\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    train_dataset = torchvision.datasets.ImageFolder(root=\"./data/oxford_iiit_pet/train\", transform=transform)\n",
    "    val_dataset = torchvision.datasets.ImageFolder(root=\"./data/oxford_iiit_pet/test\", transform=transform)\n",
    "    in_channels = 3\n",
    "    num_classes = 37\n",
    "    return train_dataset, val_dataset, in_channels, num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T15:02:36.140020Z",
     "iopub.status.busy": "2025-04-01T15:02:36.139579Z",
     "iopub.status.idle": "2025-04-01T15:02:36.150121Z",
     "shell.execute_reply": "2025-04-01T15:02:36.148551Z",
     "shell.execute_reply.started": "2025-04-01T15:02:36.139982Z"
    },
    "id": "rNTnnZtUqxf3"
   },
   "outputs": [],
   "source": [
    "def get_datasets(dataset_name, batch_size):\n",
    "    \"\"\"Return the training and validation datasets along with input channels and number of classes.\"\"\"\n",
    "    datasets = {\n",
    "        \"cifar10\": cifar10,\n",
    "        \"cifar100\": cifar100,\n",
    "        \"stanford_cars\": stanford_cars,\n",
    "        \"food101\": food101,\n",
    "        \"oxford_iiit_pet\": oxford_iiit_pet\n",
    "    }\n",
    "    if dataset_name in datasets:\n",
    "        train_dataset, val_dataset, in_channels, num_classes = datasets[dataset_name]()\n",
    "    else:\n",
    "        raise ValueError(f\"Dataset '{dataset_name}' is not supported.\")\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "    return train_loader, val_loader, in_channels, num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VVhHp2HMkqmH"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2025-04-01T15:02:36.152886Z",
     "iopub.status.busy": "2025-04-01T15:02:36.152403Z",
     "iopub.status.idle": "2025-04-01T15:02:36.164504Z",
     "shell.execute_reply": "2025-04-01T15:02:36.161768Z",
     "shell.execute_reply.started": "2025-04-01T15:02:36.152843Z"
    },
    "id": "e3paAqmTk2P-"
   },
   "outputs": [],
   "source": [
    "#@title Train epoch\n",
    "\n",
    "def train_epoch(model, train_loader, optimizer, criterion, device, epoch, total_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, targets in tqdm(train_loader, desc=f\"Training Epoch {epoch}/{total_epochs}\", leave=False):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    return running_loss / len(train_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2025-04-01T15:02:36.167288Z",
     "iopub.status.busy": "2025-04-01T15:02:36.166414Z",
     "iopub.status.idle": "2025-04-01T15:02:36.177276Z",
     "shell.execute_reply": "2025-04-01T15:02:36.175991Z",
     "shell.execute_reply.started": "2025-04-01T15:02:36.167227Z"
    },
    "id": "PkQs_MsXk77n"
   },
   "outputs": [],
   "source": [
    "#@title Validate\n",
    "\n",
    "def validate(model, val_loader, criterion, device, epoch, total_epochs):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in tqdm(val_loader, desc=f\"Validation Epoch {epoch}/{total_epochs}\", leave=False):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == targets).sum().item()\n",
    "    avg_loss = running_loss / len(val_loader.dataset)\n",
    "    accuracy = correct / len(val_loader.dataset)\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1wmvEE_blE-G"
   },
   "source": [
    "# Runing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2025-04-01T15:02:36.190976Z",
     "iopub.status.busy": "2025-04-01T15:02:36.189850Z",
     "iopub.status.idle": "2025-04-01T15:02:36.205257Z",
     "shell.execute_reply": "2025-04-01T15:02:36.203779Z",
     "shell.execute_reply.started": "2025-04-01T15:02:36.190908Z"
    },
    "id": "jO1JiJlcm8rk"
   },
   "outputs": [],
   "source": [
    "#@title Main function\n",
    "def main():\n",
    "    train_loader, val_loader, in_channels, num_classes = get_datasets(dataset_arg, batch_size_arg)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Training on {device}\")\n",
    "    results = {}\n",
    "\n",
    "    # Map model names to their constructors with appropriate in_channels and num_classes.\n",
    "    model_dict = {\n",
    "        \"relukannet\": lambda: ReluKANNetB0(in_channels=in_channels, num_classes=num_classes, g=g_arg, k=k_arg, depth_list=depth_list_arg, channel_configs=channel_configs_arg),\n",
    "        \"mobilenetv2_100\": lambda: timm.create_model(\"mobilenetv2_100\", pretrained=False, num_classes=num_classes),\n",
    "        \"efficientnet_lite0\": lambda: timm.create_model(\"efficientnet_lite0\", pretrained=False, num_classes=num_classes),\n",
    "        \"resnet18\": lambda: timm.create_model(\"resnet18\", pretrained=False, num_classes=num_classes),\n",
    "        \"resnet34\": lambda: timm.create_model(\"resnet34\", pretrained=False, num_classes=num_classes),\n",
    "        #\"resnet50\": lambda: timm.create_model(\"resnet50\", pretrained=False, num_classes=num_classes),\n",
    "        \"vit_tiny_patch16_224\": lambda: timm.create_model(\"vit_tiny_patch16_224\", pretrained=False, num_classes=num_classes),\n",
    "        #\"vit_base_patch16_224\": lambda: timm.create_model(\"vit_base_patch16_224\", pretrained=False, num_classes=num_classes),\n",
    "        #\"convnext_base\": lambda: timm.create_model(\"convnext_base\", pretrained=False, num_classes=num_classes),\n",
    "    }\n",
    "\n",
    "    models_arg=[model_arg] # To repalce args\n",
    "\n",
    "    for model_name in models_arg:\n",
    "        if model_name not in model_dict:\n",
    "            print(f\"Model '{model_name}' not recognized. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nTraining model: {model_name}\")\n",
    "        model = model_dict[model_name]()\n",
    "        model.to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr_arg)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # To track training progress.\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        val_accuracies = []\n",
    "\n",
    "        for epoch in range(1, epochs_arg + 1):\n",
    "            train_loss = train_epoch(model, train_loader, optimizer, criterion, device, epoch, epochs_arg)\n",
    "            val_loss, val_accuracy = validate(model, val_loader, criterion, device, epoch, epochs_arg)\n",
    "            print(\n",
    "                f\"Epoch {epoch}/{epochs_arg} - Train Loss: {train_loss:.4f} | \"\n",
    "                f\"Val Loss: {val_loss:.4f} | Val Acc: {val_accuracy:.4f}\"\n",
    "            )\n",
    "            train_losses.append(train_loss)\n",
    "            val_losses.append(val_loss)\n",
    "            val_accuracies.append(val_accuracy)\n",
    "\n",
    "\n",
    "        results[model_name] = {\n",
    "            \"metrics\": {\n",
    "                \"train_loss\": train_losses,\n",
    "                \"val_loss\": val_losses,\n",
    "                \"val_accuracy\": val_accuracies,\n",
    "            },\n",
    "            \"number_of_parameters\": sum(p.numel() for p in model.parameters()),\n",
    "        }\n",
    "\n",
    "        with open(f'./results/{output_arg}.json', 'w') as f:\n",
    "            json.dump(results, f, indent=4)\n",
    "        print(f\"\\nTraining results saved to {output_arg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_param_count():\n",
    "    # Map model names to their constructors with appropriate in_channels and num_classes.\n",
    "    model_dict = {\n",
    "        \"relukannet\": lambda: ReluKANNetB0(in_channels=in_channels, num_classes=num_classes, g=g_arg, k=k_arg, depth_list=depth_list_arg, channel_configs=channel_configs_arg),\n",
    "        \"mobilenetv2_100\": lambda: timm.create_model(\"mobilenetv2_100\", pretrained=False, num_classes=num_classes),\n",
    "        \"efficientnet_lite0\": lambda: timm.create_model(\"efficientnet_lite0\", pretrained=False, num_classes=num_classes),\n",
    "        \"resnet18\": lambda: timm.create_model(\"resnet18\", pretrained=False, num_classes=num_classes),\n",
    "        \"resnet34\": lambda: timm.create_model(\"resnet34\", pretrained=False, num_classes=num_classes),\n",
    "        \"vit_tiny_patch16_224\": lambda: timm.create_model(\"vit_tiny_patch16_224\", pretrained=False, num_classes=num_classes),\n",
    "    }\n",
    "\n",
    "    if model_arg not in model_dict:\n",
    "        print(f\"Model '{model_arg}' not recognized.\")\n",
    "        return\n",
    "\n",
    "    # Get dataset details\n",
    "    _, _, in_channels, num_classes = get_datasets(dataset_arg, batch_size_arg)\n",
    "\n",
    "    # Initialize the model\n",
    "    model = model_dict[model_arg]()\n",
    "    \n",
    "    # Calculate and print the number of parameters\n",
    "    param_count = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "    print(f\"Model '{model_arg}' has {param_count} parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-04-01T15:06:58.378747Z",
     "iopub.status.busy": "2025-04-01T15:06:58.377365Z"
    },
    "id": "1qwjCyuItK5m",
    "outputId": "74c9ab86-d0c0-4fd9-d9f3-4bae479cda0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 'relukannet' has 29386 parameters.\n"
     ]
    }
   ],
   "source": [
    "#@title Args\n",
    "dataset_arg=\"cifar10\" #@param [\"cifar10\", \"cifar100\", \"stanford_cars\", \"food101\", \"oxford_iiit_pet\"]\n",
    "model_arg = \"relukannet\" #@param [\"relukannet\",\"mobilenetv2_100\",\"efficientnet_lite0\",\"resnet18\",\"resnet34\",\"vit_tiny_patch16_224\"]\n",
    "\n",
    "batch_size_arg = 36 # @param {\"type\":\"slider\",\"min\":1,\"max\":64,\"step\":1}\n",
    "epochs_arg = 100 #@param\n",
    "lr_arg = 0.001 #@param\n",
    "\n",
    "output_arg = \"kannet_t1\" # @param {\"type\":\"string\"}\n",
    "\n",
    "g_arg = 3 #@param\n",
    "k_arg = 3 #@param\n",
    "\n",
    "depth_list_arg = [2, 2, 1, 1] #@param\n",
    "channel_configs_arg = [4, 8, 10, 12] #@param\n",
    "\n",
    "#main()\n",
    "print_model_param_count()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "xJBPAVwJkYCH",
    "VVhHp2HMkqmH"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
